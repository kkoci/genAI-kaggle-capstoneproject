{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb085fe4-8817-418e-843a-aca7404d5476",
   "metadata": {},
   "source": [
    "# üìò Capstone Project: Voice Cloning with Emotion Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c503dc-f263-44d3-85ed-ad542de39e0c",
   "metadata": {},
   "source": [
    "# üß† Project Title\n",
    "\n",
    "**Democratizing Voice Identity Preservation: An Ethical AI Tool for Accessibility and Creative Expression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf337059-945d-4e98-b0a3-3293933147f7",
   "metadata": {},
   "source": [
    "# üí° Use Cases & Problems Solved\n",
    "\n",
    "## 1. Accessibility Solutions\n",
    "\n",
    "- **Speech Impairments**: Clone pre-recorded voices for ALS, throat cancer, or vocal cord injury patients to maintain vocal identity.\n",
    "- **Aging Populations**: Help elderly users preserve their natural speech patterns.\n",
    "\n",
    "> **Example**: A Parkinson‚Äôs patient uses old recordings to generate speech in their own voice.\n",
    "\n",
    "## 2. Content Creation & Media\n",
    "\n",
    "- **Voice Consistency**: Podcasters/YouTubers fix errors without re-recording.\n",
    "- **Multilingual Dubbing**: Retain original speaker identity across languages.\n",
    "\n",
    "> **Example**: A YouTuber adds corrections without tonal shifts in post-production.\n",
    "\n",
    "## 3. Personalized AI Assistants\n",
    "\n",
    "- **Generic Voices**: Overcome limitations of Siri, Alexa, etc.\n",
    "\n",
    "> **Example**: Users create assistants that sound like themselves or loved ones.\n",
    "\n",
    "## 4. Education & Language Learning\n",
    "\n",
    "- **Pronunciation Practice**: Learners compare correct pronunciation using their own cloned voice.\n",
    "- **Preserving Indigenous Languages**: Native speakers clone their voice to pass on endangered languages.\n",
    "\n",
    "> **Example**: A language app compares user's cloned voice with native speaker samples.\n",
    "\n",
    "## 5. Entertainment & Gaming\n",
    "\n",
    "- **NPC Voice Diversity**: Generate character voices efficiently.\n",
    "- **Cost Reduction**: Indie studios save money by cloning minor character voices.\n",
    "\n",
    "> **Example**: RPG games use your voice to dynamically generate avatar dialogues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd915d27-8efb-403a-8b48-5fed3bc90636",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Ethical Considerations\n",
    "\n",
    "- **Deepfake Risks**: Watermarking & consent verification needed.\n",
    "- **Bias Mitigation**: Ensure fair performance across accents and genders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2637813-124c-40ff-8278-30fee02a80af",
   "metadata": {},
   "source": [
    "# üß™ Key Differentiators\n",
    "\n",
    "- **Few-shot voice cloning** (no need for hours of training data)  \n",
    "- **Emotion control**, rare in open-source  \n",
    "- **Low hardware requirements**, runs on consumer laptops  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf01ba-3b94-48cc-ba5b-f9c0d884fc7f",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Setup (Run This First)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abb470-12f8-46b7-af0b-0a2fe357249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install TTS==0.20.2 gradio==3.50.2 pydub==0.25.1 torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers soundfile resemblyzer scikit-learn pyaudio\n",
    "\n",
    "# Linux equivalents for FFmpeg and Espeak (replace with appropriate commands if using other distros)\n",
    "!sudo apt update\n",
    "!sudo apt install ffmpeg espeak -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3cb20-8c0b-4c2a-80d4-41b76e7c0aa8",
   "metadata": {},
   "source": [
    "# üß† Voice Cloning with Emotion Control - Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca196f-f63d-4fab-9d9e-68900fcfde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import gradio as gr\n",
    "from TTS.api import TTS\n",
    "from pydub import AudioSegment\n",
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import traceback\n",
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "from transformers import pipeline as nlp_pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f256d7-3984-4453-868d-2b78893daade",
   "metadata": {},
   "source": [
    "# üìç Emotion Detection from Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491d543-c854-43ac-9e32-7ed35a499edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_emotion(text):\n",
    "    \"\"\"Predict emotion from text using NLP\"\"\"\n",
    "    try:\n",
    "        classifier = nlp_pipeline(\n",
    "            \"text-classification\", \n",
    "            model=\"SamLowe/roberta-base-go_emotions\",\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        result = classifier(text)[0]\n",
    "        emotion_map = {\n",
    "            \"joy\": \"happy\",\n",
    "            \"sadness\": \"sad\",\n",
    "            \"anger\": \"angry\",\n",
    "            \"neutral\": \"neutral\"\n",
    "        }\n",
    "        return emotion_map.get(result['label'].lower(), \"neutral\")\n",
    "    except Exception as e:\n",
    "        print(f\"Text emotion detection failed: {e}\")\n",
    "        return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a778594-e05e-4917-a25e-25a24dc4c47e",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Emotion Detection from Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574e157-a6bc-4e62-b521-0dd9f41d0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion(audio_path):\n",
    "    \"\"\"Detect emotion from audio using wav2vec2\"\"\"\n",
    "    try:\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        if len(audio.shape) > 1:  # Convert stereo to mono\n",
    "            audio = np.mean(audio, axis=1)\n",
    "        \n",
    "        classifier = pipeline(\n",
    "            \"audio-classification\", \n",
    "            model=\"superb/wav2vec2-base-superb-er\",\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        predictions = classifier(audio, sampling_rate=sr)\n",
    "        return predictions[0]['label'].lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion detection error: {e}\")\n",
    "        return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76223b2-1e46-433a-a30f-f0398cbefad1",
   "metadata": {},
   "source": [
    "# üîÅ Voice Similarity Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac462b-70d4-482c-a7ca-7de1cb0c95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_similarity(original_path, cloned_path):\n",
    "    \"\"\"Calculate voice similarity score (0-1)\"\"\"\n",
    "    try:\n",
    "        encoder = VoiceEncoder()\n",
    "        orig_embed = encoder.embed_utterance(preprocess_wav(original_path))\n",
    "        clone_embed = encoder.embed_utterance(preprocess_wav(cloned_path))\n",
    "        return float(cosine_similarity([orig_embed], [clone_embed])[0][0])\n",
    "    except Exception as e:\n",
    "        print(f\"Similarity evaluation failed: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7502814-f3f3-4257-8f21-6aae0d234a52",
   "metadata": {},
   "source": [
    "# üß¨ Voice Cloning Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99e515-9845-40fa-9323-bcdb2704217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_voice(text, audio_path1, audio_path2=None, audio_path3=None, target_emotion=\"\"):\n",
    "    try:\n",
    "        # Process reference audios\n",
    "        ref_paths = []\n",
    "        for i, path in enumerate([p for p in [audio_path1, audio_path2, audio_path3] if p]):\n",
    "            audio = AudioSegment.from_file(path)\n",
    "            processed_path = os.path.join(tempfile.gettempdir(), f\"processed_{i}.wav\")\n",
    "            audio.set_channels(1).set_frame_rate(22050).normalize().export(\n",
    "                processed_path, format=\"wav\", codec=\"pcm_s16le\"\n",
    "            )\n",
    "            ref_paths.append(processed_path)\n",
    "        \n",
    "        if not ref_paths:\n",
    "            raise ValueError(\"At least one reference audio required\")\n",
    "\n",
    "        # Determine emotion\n",
    "        emotion_options = {\n",
    "            \"\": extract_emotion(ref_paths[0]),  # Auto-detect\n",
    "            \"happy\": \"happy\",\n",
    "            \"sad\": \"sad\",\n",
    "            \"angry\": \"angry\",\n",
    "            \"neutral\": \"neutral\"\n",
    "        }\n",
    "        if target_emotion == \"\":\n",
    "            final_emotion = detect_text_emotion(text)\n",
    "            print(f\"Text suggests emotion: {final_emotion}\")\n",
    "        else:\n",
    "            final_emotion = emotion_options.get(target_emotion.lower(), \"neutral\")\n",
    "        print(f\"Using emotion: {final_emotion}\")\n",
    "\n",
    "        # Generate voice clone\n",
    "        tts = TTS(\"tts_models/multilingual/multi-dataset/your_tts\").to(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        output_path = os.path.join(tempfile.gettempdir(), \"output.wav\")\n",
    "        tts.tts_to_file(\n",
    "            text=text,\n",
    "            speaker_wav=ref_paths,\n",
    "            file_path=output_path,\n",
    "            language=\"en\",\n",
    "            emotion=final_emotion,\n",
    "            use_speaker_embedding=True\n",
    "        )\n",
    "\n",
    "        # Validate emotion\n",
    "        cloned_emotion = extract_emotion(output_path)\n",
    "        emotion_valid = cloned_emotion == final_emotion\n",
    "        print(f\"Requested: {final_emotion} | Got: {cloned_emotion} | Valid: {emotion_valid}\")\n",
    "\n",
    "        return (\n",
    "            output_path,\n",
    "            evaluate_similarity(ref_paths[0], output_path),\n",
    "            f\"{final_emotion} (‚úî)\" if emotion_valid else f\"{final_emotion} (‚úñ Got {cloned_emotion})\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        error_path = os.path.join(tempfile.gettempdir(), \"error.wav\")\n",
    "        AudioSegment.silent(duration=1000).export(error_path, format=\"wav\")\n",
    "        return error_path, 0.0, \"Error\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0b558-f5da-4233-afcc-a114cca61054",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Gradio Web UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fcf42-bb74-4856-a8db-97f2d1365eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=clone_voice,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Text to Speak\", placeholder=\"Hello world...\"),\n",
    "            gr.Audio(label=\"Primary Voice\", sources=[\"upload\"], type=\"filepath\"),\n",
    "            gr.Audio(label=\"Extra Voice 1 (Optional)\", sources=[\"upload\"], type=\"filepath\"),\n",
    "            gr.Audio(label=\"Extra Voice 2 (Optional)\", sources=[\"upload\"], type=\"filepath\"),\n",
    "            gr.Dropdown(\n",
    "                choices=[\"\", \"happy\", \"sad\", \"angry\", \"neutral\"],\n",
    "                value=\"\",\n",
    "                label=\"Force Emotion (empty=auto)\"\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Audio(label=\"Cloned Voice\"),\n",
    "            gr.Number(label=\"Similarity Score (0-1)\"),\n",
    "            gr.Label(label=\"Emotion Validation\")\n",
    "        ],\n",
    "        title=\"Advanced Voice Cloner with Emotion Control\",\n",
    "        description=\"Upload 1-3 voice samples + text. Emotions: happy/sad/angry/neutral\",\n",
    "        allow_flagging=\"never\"\n",
    "    )\n",
    "    iface.launch(server_port=7860)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a8f79-7bb6-4eee-b39e-7dc13ecadd22",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Remarks\n",
    "\n",
    "This capstone project demonstrates how few-shot voice cloning with emotion control can serve as a powerful tool across accessibility, education, entertainment, and creative media. By addressing both the technical challenges and ethical considerations, the solution aims to **democratize voice identity preservation**‚Äîgiving people greater control over how their voices are used, shared, and remembered.\n",
    "\n",
    "### ‚ú® Key Takeaways:\n",
    "- **Real-World Applications**: From empowering ALS patients to enabling cost-effective voice production in indie games, the technology has broad societal value.\n",
    "- **Ethical AI in Practice**: This project prioritizes **consent, bias mitigation, and watermarking**, offering a responsible approach to voice cloning.\n",
    "- **Technical Innovation**: By combining state-of-the-art NLP, TTS, and audio processing pipelines, this tool runs on consumer-grade hardware with minimal voice data required.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Future Directions\n",
    "- Implement **speaker diarization** for multi-speaker voice separation.\n",
    "- Add a **web interface for live cloning previews**.\n",
    "- Support **language switching** with accent preservation.\n",
    "- Explore **emotion blending** and **style transfer** (e.g., sarcasm, excitement).\n",
    "\n",
    "---\n",
    "\n",
    "Thank you for reviewing this capstone. Feedback and collaboration are welcome! üí¨\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
